{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b4b919-e5f2-4ffa-8d5f-331a29ec010f",
   "metadata": {},
   "source": [
    "# Access to Prompt data\n",
    "\n",
    "- creationn date : 2025-12-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47915567-8609-4cc6-a2f3-f2750a1b28cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T11:41:57.461949Z",
     "iopub.status.busy": "2025-12-04T11:41:57.461689Z",
     "iopub.status.idle": "2025-12-04T11:42:02.231646Z",
     "shell.execute_reply": "2025-12-04T11:42:02.230911Z",
     "shell.execute_reply.started": "2025-12-04T11:41:57.461930Z"
    }
   },
   "outputs": [],
   "source": [
    "from lsst.daf.butler import Butler\n",
    "from lsst.daf.butler import CollectionType\n",
    "from lsst.daf.butler import DatasetType\n",
    "import textwrap\n",
    "\n",
    "try:\n",
    "    from lsst.daf.butler.exceptions import DatasetTypeError\n",
    "    print(\"Import de 'DatasetTypeError' r√©ussi depuis lsst.daf.butler.exceptions.\")\n",
    "except ImportError:\n",
    "    # Si le sous-module 'exceptions' n'existe pas ou l'exception a √©t√© renomm√©e/d√©plac√©e,\n",
    "    # nous allons devoir capturer l'exception parent la plus g√©n√©rale lev√©e par le Butler.\n",
    "    print(\"Avertissement : 'DatasetTypeError' non trouv√© dans lsst.daf.butler.exceptions. Utilisation d'une gestion d'erreurs plus g√©n√©rale.\")\n",
    "    \n",
    "    # D√©finir l'exception de rechange (un alias vers un type d'erreur plus g√©n√©rique du Butler)\n",
    "    # L'exception qui est lev√©e est souvent une sous-classe de RuntimeError ou d'une autre erreur du Butler.\n",
    "    # Pour le test de getDatasetType, nous allons tenter de capturer l'exception directement.\n",
    "    pass # Nous n'avons pas besoin de d√©finir un alias si nous g√©rons l'exception apr√®s la tentative.\n",
    "\n",
    "# --- Le reste de votre code (en utilisant l'exception directement apr√®s la tentative) ---\n",
    "\n",
    "\n",
    "# Ouvrir le repo\n",
    "#repo = \"/repo/main\"\n",
    "REPO_URI = \"/repo/main\"\n",
    "butler = Butler(REPO_URI)\n",
    "\n",
    "# Lister toutes les collections et filtrer celles contenant \"LSSTCam/prompt\"\n",
    "collections = butler.registry.queryCollections()\n",
    "\n",
    "# find collections which has prompt in it\n",
    "prompt_colls = [c for c in collections if \"LSSTCam/prompt\" in c]\n",
    "\n",
    "\n",
    "# 1. Pr√©-filtrage : Cr√©e une nouvelle liste sans les collections contenant 'config'\n",
    "filtered_colls = [c for c in prompt_colls if 'config' not in c]\n",
    "\n",
    "# 2. It√©ration sur la liste filtr√©e\n",
    "for c in filtered_colls:\n",
    "    ctype = butler.registry.getCollectionType(c)\n",
    "    print(f\"{c:90s} {CollectionType(ctype).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ce72d-57e0-45e2-8bca-dfcd9f1cd16f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T11:42:29.482857Z",
     "iopub.status.busy": "2025-12-04T11:42:29.482560Z",
     "iopub.status.idle": "2025-12-04T11:42:29.682449Z",
     "shell.execute_reply": "2025-12-04T11:42:29.681605Z",
     "shell.execute_reply.started": "2025-12-04T11:42:29.482836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Le REPO_URI est le chemin du d√©p√¥t (comme dans votre commande bash)\n",
    "#REPO_URI = \"/repo/main\"\n",
    "\n",
    "# La collection de donn√©es sp√©cifique √† cibler\n",
    "#COLLECTION_NAME = 'LSSTCam/prompt/output-2025-11-04/SingleFrame/pipelines-682fa38-config-8f017ea'\n",
    "COLLECTION_NAME = \"LSSTCam/prompt/output-2025-11-20\"\n",
    "\n",
    "# Initialisation du Butler\n",
    "butler = Butler(REPO_URI, collections=[COLLECTION_NAME])\n",
    "\n",
    "print(f\"Butler initialis√© pour la collection : {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1036e7c-7c06-4510-a8ac-44437f5dbb18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T11:42:32.245879Z",
     "iopub.status.busy": "2025-12-04T11:42:32.245632Z",
     "iopub.status.idle": "2025-12-04T11:42:32.417934Z",
     "shell.execute_reply": "2025-12-04T11:42:32.417149Z",
     "shell.execute_reply.started": "2025-12-04T11:42:32.245861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assurez-vous d'avoir bien initialis√© le Butler (Butler est d√©j√† initialis√© dans le contexte)\n",
    "registry = butler.registry\n",
    "dataset_types = list(registry.queryDatasetTypes())\n",
    "\n",
    "COLLECTION_NAME = butler.collections[0] # R√©cup√®re la collection par d√©faut\n",
    "#COLLECTION_NAME = butler.collection.default\n",
    "\n",
    "print(f\"La collection '{COLLECTION_NAME}' contient {len(dataset_types)} types de datasets.\")\n",
    "\n",
    "# Afficher les noms des types de datasets, en filtrant ceux li√©s √† la d√©tection (DIA)\n",
    "print(\"\\nTypes de Datasets pertinents (contenant 'DIA' ou 'Source') :\")\n",
    "\n",
    "\n",
    "# --- NOUVEAU FILTRE : Types de donn√©es pertinents ---\n",
    "# Classes de stockage que nous voulons conserver (tables de donn√©es)\n",
    "#ALLOWED_STORAGE_CLASSES = ['DataFrame', 'SourceCatalog', 'Catalog']\n",
    "ALLOWED_STORAGE_CLASSES = ['DataFrame']\n",
    "\n",
    "# Mots-cl√©s pour filtrer les datasets li√©s √† la d√©tection et √† l'alerte\n",
    "ALLOWED_KEYWORDS = ['DIA', 'Source', 'Dia', 'DiaObject','dia'] \n",
    "\n",
    "print(f\"\\nTypes de Datasets pertinents (filtr√©s par Nom et Classe de Stockage {ALLOWED_STORAGE_CLASSES}):\")\n",
    "\n",
    "\n",
    "keep_dataset_types = []\n",
    "keep_dataset_dimensions = []\n",
    "\n",
    "# loop on datasettypes\n",
    "for ds_type in dataset_types:\n",
    "    \n",
    "    ds_name = ds_type.name\n",
    "    \n",
    "    # Correction de l'erreur: Utilisation d'un bloc try/except pour g√©rer les \n",
    "    # \"KeyError\" qui se produisent lorsque le Butler ne peut pas r√©soudre la classe de stockage (ex: 'SpectractorWorkspace').\n",
    "    storage_class_name = \"N/A\"\n",
    "    try:\n",
    "        if ds_type.storageClass:\n",
    "            storage_class_name = ds_type.storageClass.name\n",
    "    except KeyError:\n",
    "        # La classe existe dans le registre mais le module Python n'est pas charg√©\n",
    "        storage_class_name = \"UNRESOLVED_CLASS\" \n",
    "    except Exception as e:\n",
    "        # Pour tout autre type d'erreur\n",
    "        print(f\"Avertissement: √âchec de la r√©solution de la classe de stockage pour {ds_name}. Erreur: {e}\")\n",
    "        \n",
    "    \n",
    "    # Filtre 1: Doit contenir un mot-cl√© pertinent dans le nom\n",
    "    # Utilisation de .upper() pour une comparaison insensible √† la casse dans le nom du dataset\n",
    "    is_relevant_keyword = any(keyword.upper() in ds_name.upper() for keyword in ALLOWED_KEYWORDS)\n",
    "    \n",
    "    # Filtre 2: Doit √™tre une classe de stockage de type catalogue/table\n",
    "    is_relevant_storage = storage_class_name in ALLOWED_STORAGE_CLASSES\n",
    "    \n",
    "    if is_relevant_keyword and is_relevant_storage:\n",
    "        # Utilisation de textwrap pour g√©rer les noms de datasets longs sans tronquer la sortie\n",
    "        wrapped_name = textwrap.shorten(ds_name, width=40, placeholder='...')\n",
    "        print(f\"  - **{wrapped_name:40s}** : Stock√© comme un {storage_class_name}\")\n",
    "        keep_dataset_types.append(ds_name)\n",
    "        required_dimensions = list(ds_type.dimensions.names)\n",
    "        keep_dataset_dimensions.append(required_dimensions)\n",
    "\n",
    "print(\"\\n--- Liste des catalogues √† utiliser pour l'extraction de donn√©es : ---\")\n",
    "print(keep_dataset_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485f4cc-7e50-4bd2-afc5-42654272b4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5ca89-cbdb-419a-ab6e-8650a2ce4335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T11:45:47.264778Z",
     "iopub.status.busy": "2025-12-04T11:45:47.264483Z",
     "iopub.status.idle": "2025-12-04T11:46:07.165804Z",
     "shell.execute_reply": "2025-12-04T11:46:07.165047Z",
     "shell.execute_reply.started": "2025-12-04T11:45:47.264755Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Butler initialis√©. Les collections par d√©faut sont : {butler.collections}\")\n",
    "print(f\"Le Butler utilise la collection par d√©faut : {butler.collections[0]}\")\n",
    "\n",
    "for dataset_type_name in keep_dataset_types:\n",
    "\n",
    "    print(f\"Test de l'existence du DatasetType '{dataset_type_name}'...\")\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # üí° √âTAPE 1 : V√âRIFIER L'EXISTENCE DU DATASET TYPE\n",
    "    # ----------------------------------------------------\n",
    "    try:\n",
    "        # Tente de r√©cup√©rer l'objet DatasetType. \n",
    "        dt = butler.registry.getDatasetType(dataset_type_name)\n",
    "        required_dimensions = dt.dimensions.names\n",
    "        print(f\"‚úÖ Le type '{dt}' est pr√©sent et stock√© comme : {dt.storageClass.name}, required dimensions : {required_dimensions}\")\n",
    "     \n",
    "    \n",
    "        # ... Les √©tapes 2 et 3 (queryDataIds et butler.get) suivent ici ...\n",
    "\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # üí° √âTAPE 2 : S'IL EXISTE, CHERCHER LES DATAIDS\n",
    "        # ----------------------------------------------------\n",
    "        print(\"Tentative de recherche d'identifiants de donn√©es (DataIds)...\")\n",
    "\n",
    "        # Recherche des DataIds existants pour ce type de dataset\n",
    "        #valid_diasource_ids = list(butler.registry.queryDataIds(datasets=diasource_type, dimensions=['visit', 'detector', 'band']))\n",
    "        #valid_diasource_ids = list(butler.registry.queryDataIds(datasets=diasource_type),dimensions=required_dimensions,limit=1)  # Suppression du param√®tre 'dimensions'\n",
    "        #valid_diasource_ids = list(butler.registry.queryDataIds(datasets=diasource_type),dimensions=required_dimensions,limit=1)\n",
    "\n",
    "        valid_data_ids = list(butler.registry.queryDataIds(\n",
    "        datasets=dataset_type_name, \n",
    "        dimensions=required_dimensions, # <- Correctement pass√©\n",
    "        ).limit(1))\n",
    "\n",
    "        if valid_data_ids:\n",
    "            print(f\"‚úÖ {len(valid_data_ids)} combinaisons visit/detector/band trouv√©es.\")\n",
    "\n",
    "            #required_dimensions = list(ds_type_obj.dimensions.getDimensionNames())\n",
    "            \n",
    "            # Choisir le premier dataId trouv√©\n",
    "            #target_data_id = valid_diasource_ids[0].butlerDataId\n",
    "            target_data_id = valid_data_ids[0].getButlerDataId()\n",
    "            print(f\"DataId cible pour le chargement : {target_data_id}\")\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # üí° √âTAPE 3 : TENTER LE CHARGEMENT\n",
    "            # ----------------------------------------------------\n",
    "            try:\n",
    "                diasource_catalogue = butler.get(dataset_type_name, dataId=target_data_id)\n",
    "                print(\"\\n--- ‚úÖ‚úÖ Chargement dataset  R√©ussi ---\")\n",
    "                print(f\"‚úÖ‚úÖ Catalogue charg√© avec **{len(diasource_catalogue)}** entr√©es.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                # G√®re les erreurs qui pourraient survenir lors de la lecture physique (I/O, corruption, etc.)\n",
    "                print(f\"‚ùå Erreur I/O ou de lecture physique (butler.get()) : {e}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ùå Attention : Le type '{dataset_type_name}' existe, mais aucune donn√©e n'a √©t√© produite (0 DataId trouv√©e).\")\n",
    "\n",
    "        \n",
    "\n",
    "    # Si l'importation de DatasetTypeError a r√©ussi, vous pouvez l'utiliser :\n",
    "    # except DatasetTypeError: \n",
    "    #     ...\n",
    "\n",
    "    # Alternativement (et plus robuste sans importation r√©ussie), \n",
    "    # capturer l'exception sp√©cifique lev√©e par la m√©thode getDatasetType\n",
    "    # ou une erreur plus g√©n√©rale si la premi√®re √©choue.\n",
    "\n",
    "    except Exception as e:\n",
    "        # Si getDatasetType l√®ve une exception (comme une RuntimeError ou l'exception non import√©e)\n",
    "        # qui contient le message que le dataset type n'existe pas, nous la g√©rons ici.\n",
    "    \n",
    "        # La m√©thode la plus s√ªre est de v√©rifier le message de l'erreur lev√©e par le Butler, \n",
    "        # car l'erreur est lev√©e au moment o√π getDatasetType √©choue.\n",
    "        if f\"Dataset type '{dataset_type_name}' does not exist\" in str(e):\n",
    "             print(f\"‚ùå Le type de dataset '{diasource_type}' n'existe pas dans la collection par d√©faut (Erreur captur√©e : {type(e).__name__}).\")\n",
    "             print(\"Veuillez choisir une collection issue d'un pipeline d'alertes (ApPipe complet).\")\n",
    "        else:\n",
    "             # G√©rer une autre erreur inattendue\n",
    "             raise e\n",
    "# ----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e271ae-130f-41a2-890c-182a447271ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
